{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "p51HjixfUCHG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "3ffe4916-4802-4d61-f173-7a1e3b92a4de"
      },
      "source": [
        "%%time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import math\n",
        "import time\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy\n",
        "import scipy.sparse\n",
        "from tqdm import tqdm,tqdm_notebook\n",
        "from contextlib import contextmanager\n",
        "import os\n",
        "import re\n",
        "import gc\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, Input , Dropout, Flatten,concatenate,LSTM\n",
        "from tensorflow.keras.layers import Embedding\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.models import Model,load_model\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint,TensorBoard,ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import optimizers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.53 s, sys: 206 ms, total: 1.73 s\n",
            "Wall time: 1.91 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rp-6XGKNaHVY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "# https://gist.github.com/sebleier/554280\n",
        "def decontracted(phrase):\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    return phrase\n",
        "\n",
        "stopwords = stopwords.words('english')\n",
        "\n",
        "def clean_text_data(data):\n",
        "    preprocessed = []\n",
        "    # tqdm is for printing the status bar\n",
        "    for sentance in tqdm(data):\n",
        "        sent = decontracted(sentance)\n",
        "        sent = sent.replace('\\\\r', ' ')\n",
        "        sent = sent.replace('\\\\\"', ' ')\n",
        "        sent = sent.replace('\\\\n', ' ')\n",
        "        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
        "        # https://gist.github.com/sebleier/554280\n",
        "        sent = ' '.join(e for e in sent.split() if e not in stopwords)\n",
        "        preprocessed.append(sent.lower().strip())\n",
        "    return preprocessed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlV6zoxUadt3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_cat_data(cat_column):\n",
        "    '''takes categorical column values as arguments and returns list of cleaned categories'''\n",
        "#cleaning is necessary because:\n",
        "#we do each column analysis and remove if there are any special symbols like & and replace all those and spaces with '_',so that if we encounter any test data point with & symbol,it should not be treated as new point\n",
        "#for example: if we train our model with computer&mobiles and if we encounter a category computer mobiles,while predicting model should not treat it as new category,so better do all analysis\n",
        "#here we have 3 million test data points to test our model,so these one or two points may not significantly improve our test score,so that is why here we can exclude all the further analysis,but in real world, where each test point outcome is important we have to do all the detailed analysis\n",
        "    cat = list(cat_column)\n",
        "    cat_list = []\n",
        "    for i in tqdm(cat):\n",
        "        i = re.sub('[^A-Za-z0-9]+', ' ', i)\n",
        "        i = i.replace(' ','_')\n",
        "        i = i.replace('-','_')\n",
        "        i = i.replace(' & ','_')\n",
        "        i = i.lower()\n",
        "        cat_list.append(i.strip())\n",
        "    \n",
        "    return cat_list"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qtGPnk6YuJg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def concat(x): #concatenating all the text columns and categorical columns seperately by replacing null values with space\n",
        "  x['name'] = x['name'].fillna('') + ' ' + x['brand_name'].fillna('')\n",
        "  x['text'] = (x['item_description'].fillna('') + ' ' + x['name'] + ' ' + x['category_name'].fillna(''))\n",
        "  x['cat']  = x['category_name'].fillna('') + ' ' + x['brand_name'].fillna('')\n",
        "  return x[['name', 'text','cat','shipping', 'item_condition_id']]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVgSRrEQZpFv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_encoder(test,col):\n",
        "    if col == 'name':\n",
        "        vectorizer = pickle.load(open('name.pkl','rb')) #pre trained vectorizer on train data\n",
        "        test_transform = vectorizer.transform(test)\n",
        "        #del vectorizer\n",
        "        #gc.collect()\n",
        "        return test_transform\n",
        "    else:\n",
        "        vectorizer = pickle.load(open('text.pkl','rb'))#pre trained vectorizer on train data\n",
        "        test_transform = vectorizer.transform(test)\n",
        "        #del vectorizer\n",
        "        #gc.collect()\n",
        "    #feat_names = vectorizer.get_feature_names()\n",
        "        return test_transform"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VgwylINhtt2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cat_encoder(col):\n",
        "  vectorizer = pickle.load(open('cat.pkl','rb')) #pre trained vectorizer on train data\n",
        "  test_transform = vectorizer.transform(col)\n",
        "  return test_transform"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8O3dfw_Z0Sr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dummy_encoder(test):\n",
        "    test_transform = scipy.sparse.csr_matrix(pd.get_dummies(test[[\"item_condition_id\", \n",
        "                                                                         \"shipping\"]], sparse = True).values)\n",
        "    return test_transform"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6RiHomjnEFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This function groups the [category_name,brand_name,shipping] features and generates aggregate 'price' variable statistics like Mean,\n",
        "# Median, Std. Deviation, Price Ranges based on 2 std.deviations from mean,coefficient of variance etc. in the Log transformed scale.\n",
        "# Outputs are standardized using StandardScaler function\n",
        "def generate_category_features(cv):\n",
        "    df_group = pd.read_csv('train_cbs')\n",
        "    df_group['cbs_log_price_std'] = df_group['cbs_log_price_std'].fillna(0)\n",
        "    df_group['cbs_log_price_conf_variance'] = df_group['cbs_log_price_std'] / df_group['cbs_log_price_mean']\n",
        "    df_group['cbs_log_count'] = np.log1p(df_group['cbs_count'])\n",
        "    df_group['cbs_min_expected_log_price'] = (df_group['cbs_log_price_mean'] - (df_group['cbs_log_price_std']*2)).clip(lower=1.0)\n",
        "    df_group['cbs_max_expected_log_price'] = (df_group['cbs_log_price_mean'] + (df_group['cbs_log_price_std']*2))\n",
        "    \n",
        "    df_group_stats = cv.merge(df_group.reset_index(),\n",
        "                                      how = 'left',\n",
        "                                      on = 'cat_brand_ship')[['cbs_log_price_mean','cbs_min_expected_log_price','cbs_max_expected_log_price']].fillna(0).values\n",
        "    \n",
        "    scaler = pickle.load(open('scaler.pkl','rb'))\n",
        "    cbs_feats_scaled = scaler.transform(df_group_stats)\n",
        "    return cbs_feats_scaled"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhOR21Kvs3b-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def category_encoding(cv,col):\n",
        "  if col == 'ship':\n",
        "    l_cat = pd.read_csv('lstm_ship.csv')\n",
        "    unique = l_cat['unique'].values\n",
        "    cat_rank = l_cat['cat_rank'].values\n",
        "  else:\n",
        "    l_cat = pd.read_csv('lstm_cond.csv')\n",
        "    unique = l_cat['unique'].values\n",
        "    cat_rank = l_cat['cat_rank'].values\n",
        "    for category in cv:\n",
        "        if category in unique:\n",
        "            encoded_cv.append(cat_rank[category]) \n",
        "        else:\n",
        "            encoded_cv.append(0) \n",
        "    \n",
        "    encoded_cv = np.asarray(encoded_cv)\n",
        "    return encoded_cv"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1TXN9kFo2_U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_concat(df):\n",
        "    df['text'] = (df['name'].astype(str).fillna('') + ' ' + df['brand_name'].astype(str).fillna('') + ' ' + \n",
        "                df['item_description'].astype(str).fillna('') + \" \" + df['category_name'].astype(str).fillna(''))\n",
        "    return df['text']"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsAZ-dvsiB6W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This Code uses Keras tokeizer to create Word Tokens from the Text Vocabulary, it takes in train,cv text data as input, returns keras tokenizer and tokeized train,cv datasets \n",
        "def tokenize(cv):\n",
        "    tok = pickle.load(open('tokenizer.pkl','rb'))\n",
        "    #print('Total number of words in the document are ',len(t.word_index) + 1)\n",
        "    cv_tokenized = tok.texts_to_sequences(cv)\n",
        "    return cv_tokenized #return tokens and tokenizer"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGXKWuxZorl6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This Code takes train,cv data and performs Pre-padding with max_length= 100\n",
        "def padding(cv,sl):\n",
        "    padded_cv_text = pad_sequences(cv, maxlen = sl, padding='pre')\n",
        "    return padded_cv_text"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qw-YAkCUk3Mg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lstm_data(x):\n",
        "  max_len = 100 #from our basic analysis,as most of the sentences has around 100 length,we take maximum length as 100 \n",
        "\n",
        "\n",
        "  x['text'] = text_concat(x)\n",
        "  x_text = tokenize(clean_text_data(x['text']))\n",
        "  x_text_pad = padding(x_text,max_len)\n",
        "\n",
        "  x_ship = category_encoding(x['shipping'].astype('category'),'ship')\n",
        "                                                  \n",
        "  x_cond = category_encoding(x['item_condition_id'].astype('category'),'cond')\n",
        "\n",
        "\n",
        "  x['cat_brand_ship'] = (x['category_name'].astype(str) + \"_\" +  #merge all category features into single column\n",
        "                                    x['brand_name'].astype(str) + \"_\" +  \n",
        "                                    x['shipping'].astype(str))\n",
        "  x_cbs_feats = generate_category_features(x)\n",
        "\n",
        "\n",
        "  return [x_text_pad,np.array(x_ship),np.array(x_cond),x_cbs_feats]\n",
        "           \n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H80s047MyMz1",
        "colab_type": "text"
      },
      "source": [
        "Final function 1: It takes inputs (item's features) in the form of dataframe:\n",
        "[\"train_id\"-numerical,\n",
        "\"name\" - string,\n",
        "\"item_condition_id\" - numerical,\n",
        "\"category_name\" - string,\n",
        "\"brand_name\" -string,\n",
        "\"shipping\" - numerical,\n",
        "\"item_description\" - string]\n",
        "\n",
        "It returns the predicted price value for the given item's features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fiJBuiWWdhg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def final_fun_1(x):\n",
        "  #print(\"shape of the given input is:\",x.shape)\n",
        "  #cleaning and preprocessing the data\n",
        "  concatenated_x = concat(x)\n",
        "  cleaned_text = clean_text_data(concatenated_x['text'])\n",
        "  cleaned_name = clean_text_data(concatenated_x['name'])\n",
        "  cleaned_cat  = clean_cat_data(concatenated_x['cat'])\n",
        "  final_name = text_encoder(cleaned_name,'name')\n",
        "  final_text = text_encoder(cleaned_text,'text')\n",
        "  finl_cat   = cat_encoder(cleaned_cat)\n",
        "  final_dummies = dummy_encoder(pd.DataFrame({\"shipping\" : x[\"shipping\"].astype(\"category\"),\n",
        "                                                             \"item_condition_id\" : x[\"item_condition_id\"].astype(\"category\")}))\n",
        "  #final data with sparse vectors\n",
        "  final_data = scipy.sparse.hstack((final_name, final_text, final_cat, final_dummies)).tocsr().astype('float32') #final data for mlp and ridge models\n",
        "\n",
        "  final_lstm_data = lstm_data(x) #preprocess data for lstm model and get the final values\n",
        "  y_scaler = pickle.load(open('y_scaler.pkl','rb')) #load the scaler for inverse transforming on predicted values\n",
        "\n",
        "  #taking the best models and predicting on the given data\n",
        "\n",
        "  #ridge model\n",
        "  ridge = pickle.load(open('ridge.pkl','rb'))#load pre trained ridge model\n",
        "  preds = ridge.predict(final_data)[:, 0]\n",
        "  preds_ridge = np.expm1(y_scaler.inverse_transform(preds.reshape(-1, 1))[:, 0])\n",
        "  #mlp model\n",
        "  mlp = load_model('mlp.h5') #load pre trained mlp model\n",
        "  preds = mlp.predict(final_data)[:, 0]\n",
        "  preds_mlp = np.expm1(y_scaler.inverse_transform(preds.reshape(-1, 1))[:, 0])\n",
        "  #lstm model\n",
        "  lstm = load_model('lstm.h5') #load pre trained lstm model\n",
        "  preds = lstm.predict(final_lstm_data)[:, 0]\n",
        "  preds_lstm = np.expm1(y_scaler.inverse_transform(preds.reshape(-1, 1))[:, 0])\n",
        " #final prediction by combining all the above results\n",
        "  final_prediction = 0.23*preds_ridge + 0.61*preds_mlp + 0.16*preds_lstm #these are the best weights we got after experimenting with test data\n",
        "\n",
        "  return final_prediction"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuKetrSP0Ch_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "2ede5918-ad1a-4da1-c6f2-7fa6f80ca191"
      },
      "source": [
        "train = pd.read_csv('train.tsv',sep = '\\t')\n",
        "example = train[1:2]\n",
        "predicted_price = final_fun_1(example)\n",
        "print('Predicted Price for ',example.values,\"\\n is \\n\",predicted_price)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted Price for  [[1 '25 pcs NEW 7.5\"x12\" Kraft Bubble Mailers' 1\n",
            "  'Other/Office supplies/Shipping Supplies' nan 1\n",
            "  '25 pcs NEW 7.5\"x12\" Kraft Bubble Mailers Lined with bubble wrap for protection Self Sealing (peel-and-seal), adhesive keeps contents secure and tamper proof Durable and lightweight Kraft material helps save on postage Approved by UPS, FedEx, and USPS.']] \n",
            " is \n",
            " [35.37330328]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxKe4IdAx5Vl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rmsle(y, y_pred): #RMSLE function for finding the score for predicted and true values\n",
        "    y_pred[y_pred<0] = 0 #if predicted values are negative,replace them with zero https://www.kaggle.com/c/bike-sharing-demand/discussion/18942\n",
        "    s = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n",
        "    return math.sqrt((sum(s)/len(y)))"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kaG6KZtP2y1v"
      },
      "source": [
        "Final function 2: It takes inputs (item's features) in the form of dataframe:\n",
        "[\"train_id\"-numerical,\n",
        "\"name\" - string,\n",
        "\"item_condition_id\" - numerical,\n",
        "\"category_name\" - string,\n",
        "\"brand_name\" -string,\n",
        "\"shipping\" - numerical,\n",
        "\"item_description\" - string]\n",
        "\n",
        "It returns the RMSLE value for predicted price with the given item's features and actual price of the item."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgcnDlFEvb7Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def final_fun_2(x, y):\n",
        "  predicted_value = final_fun_1(x) #using function1 predict the price of an item\n",
        "  true_value = y #true value of an item\n",
        "  return rmsle(true_value,predicted_value)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUZSB_g-30PQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6d275c53-5593-47a8-ef48-4e5a4e71f3e4"
      },
      "source": [
        "train = pd.read_csv('train.tsv',sep = '\\t')\n",
        "example = train[1:2]\n",
        "true_price = [31] \n",
        "rmsle_score = final_fun_2(example,true_price)\n",
        "print('RMSLE score for given inputs is ',rmsle_score)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSLE score for given inputs is  0.12809917638064006\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
